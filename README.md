# ðŸŒ± ECA-Net-Replication â€“ Efficient Channel Attention for CNNs

This repository provides a **PyTorch-based replication** of  
**ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks**.

The focus is **understanding and implementing ECA attention mechanisms** practically,  
rather than chasing benchmark SOTA results.

- Refines feature maps with **efficient channel attention** âš¡  
- Emphasizes **which channels are important** in CNN features âœ¨  
- Extremely **lightweight & plug-and-play** for any CNN backbone ðŸ› ï¸  

**Paper reference:** [ECA-Net â€“ Wang et al., 2020](https://arxiv.org/abs/1910.03151) ðŸ“„

---

## ðŸŒŒ Overview â€“ ECA Pipeline

![ECA Overview](images/figmix.jpg)

The core idea:

> Efficiently learn **channel-wise attention** by capturing local cross-channel interactions using a fast 1D convolution, without dimensionality reduction.  

High-level procedure:

1. Extract feature maps $X \in \mathbb{R}^{C \times H \times W}$ from a CNN backbone.  
2. Apply **Global Average Pooling (GAP)** per channel â†’ $y = g(X)$.  
3. Perform **local cross-channel interaction** using 1D convolution of adaptive kernel size $k$:  
   $$\omega = \sigma(\text{C1D}_k(y))$$  
   where $\sigma$ is the sigmoid function.  
4. Multiply attention weights $\omega$ with input features to get **refined features**.  
5. Forward refined features through the remaining network.  

The module can be inserted **per convolution block** and is fully **end-to-end trainable**.

---

## ðŸ§® Attention Computation â€“ Math Essentials

### Channel Attention (ECA)
Given $y = g(X) \in \mathbb{R}^{C}$ (after GAP), ECA computes channel attention via **local interaction**:

$$
\omega_i = \sigma\Big(\sum_{j \in \Omega_i^k} w_j y_j\Big), \quad i=1,...,C
$$

Where:  
- $\Omega_i^k$ = set of $k$ neighboring channels for channel $i$  
- $w_j$ = shared learnable parameter for 1D convolution  
- $k$ = kernel size, adaptively determined based on channel dimension $C$  

### Adaptive Kernel Size
To avoid manual tuning of $k$, it is determined by a non-linear mapping:

$$
k = \psi(C) = \big|\frac{\log_2(C)}{\gamma} + \frac{b}{\gamma}\big|_\text{odd}
$$

Where:  
- $\gamma$ and $b$ are hyperparameters (e.g., $\gamma=2, b=1$)  
- $|\cdot|_\text{odd}$ rounds to the nearest odd integer  

This ensures **high-dimensional channels** interact with more neighbors while low-dimensional channels use shorter ranges.

### Feature Refinement
The final refined feature map:

$$
X' = \omega \otimes X
$$

Where $\otimes$ denotes element-wise multiplication along the channel dimension.

---

## ðŸ§  What the Module Does

- Captures **local cross-channel dependencies** efficiently  
- Avoids dimensionality reduction â†’ direct channel-weight correspondence  
- Extremely lightweight: few extra parameters and negligible GFLOPs  
- Plug-and-play in any CNN block (ResNet, MobileNetV2, etc.)  

---

## ðŸ“¦ Repository Structure

```bash
ECA-Net-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py          # Standard Conv2d wrapper
â”‚   â”‚   â”œâ”€â”€ activation.py          # sigmoid helper
â”‚   â”‚   â””â”€â”€ normalization.py       # BatchNorm for backbone
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â””â”€â”€ eca_layer.py           # GAP + 1D Conv + Sigmoid
â”‚   â”‚
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ resnet_blocks.py       # CNN block + optional ECA integration
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ eca_cnn.py             # CNN + ECA forward logic
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # gamma, b, backbone type
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                  # ECA overview figure
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
